Zero Guard

Introducing Zero Guard—a decentralized zero knowledge powered AI Red Team network

AI red team is a network of real humans to evaluate and align AI to certain values.  For example if AI is asked how to break a car, the red team would guide it to discourage the act rather than provide instructions. All major AI firms use red teams, but the practice has many issues:

- It is centralized and private. Leading to questions like is the model aligned with a company's interests or broader human values? As many red teams come from similar backgrounds, public can’t tell if AI models are thoroughly tested for cultural diversity, this adds to AI's bias issue
- Companies like OpenAI admit their red teams lack diversity and invite more to join. But joining requires sharing personal data. Can we trust them with our private info, even if it's for AI safety? Many AI teams have had privacy issues and accidental data leaks.

Red teaming is like an AI audit, we don't trust banks to audit themselves due to conflicts of interest. So, why should AI, which touches every aspect of our life be different? Plus, relying on firms to protect our personal data isn't always trustworthy

Zero Guard is is a community-driven, on-chain AI red team to promote safety and diversity, it uses ZK credentials for ensuring privacy.

Here is how it works:

For ZK credential: Users can fetch personal data from web apps, creating a proof with TLSnotary. This decentralized proof, verified on Cartesi, allows users to make a credible credential using Polygon, unlike common self-issued ones

For on-chain red teaming, users get a semaphore identity to join alignment projects so they can vote anonymously, guarding against sybil attacks. Contributors get rewarded. To prevent corruption, votes are hashed with a random salt and sent to a smart contract as a commitment. This salt, stored securely in IPFS via Lit, is only decrypted post-voting using conditional access control, to minimize voting collusion, vote count is zkp verified uing Axiom's technology
